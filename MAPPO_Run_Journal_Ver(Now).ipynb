{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e00d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /users/smostafa/.local/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from gym) (1.23.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy-stl in /users/smostafa/.local/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /users/smostafa/.local/lib/python3.10/site-packages (from numpy-stl) (3.4.5)\n",
      "Requirement already satisfied: numpy in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from numpy-stl) (1.23.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /users/smostafa/.local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /users/smostafa/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /users/smostafa/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /users/smostafa/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /users/smostafa/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: setuptools in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.3.0)\n",
      "Requirement already satisfied: wheel in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (4.64.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in /users/smostafa/.local/lib/python3.10/site-packages (2.11.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (2.16.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (3.20.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (65.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (1.43.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from tensorboard) (1.23.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /users/smostafa/.local/lib/python3.10/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /users/smostafa/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /users/smostafa/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /users/smostafa/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /users/smostafa/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /users/smostafa/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SciencePlots in /users/smostafa/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from SciencePlots) (3.5.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (1.23.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (4.37.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from matplotlib->SciencePlots) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->SciencePlots) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install numpy-stl\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "!pip install tensorboard \n",
    "!pip install SciencePlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386d6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import scienceplots\n",
    "import matplotlib\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "from plot_utils import window_mean\n",
    "import csv\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import seaborn as sns\n",
    "from misc import process_oh_actions\n",
    "from envs import MECSCHEnvV1\n",
    "from envs import MECSCHEnvV2\n",
    "from tqdm.notebook import tqdm\n",
    "from agent_wrapper import OnPolicyWrapper\n",
    "from models import MLPCategoricalActor, MLPRelaxedCategoricalActor, BaseMLPNet, BaseMLPActor\n",
    "from plot_functions import set_fonts, set_style, draw_boxplot, draw_brace\n",
    "from gym.envs import register\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from colorsys import rgb_to_hls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319813f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59c9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_envs():  \n",
    "    # 'MECSCH-v1': Mobile edge computing full computation offloading no communication NOMA with reduction\n",
    "    gym.envs.register(id='MECSCH-v1',entry_point=MECSCHEnvV2,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':True,'NOMA_Scheme':True,'OMA_Scheme':False,'Reduction':True,\n",
    "                 'Round_robin':False,'semi_static':False,'heuristic':False})\n",
    "    \n",
    "    # 'MECSCH-v2': Mobile edge computing full computation offloading no communication NOMA without reduction\n",
    "    gym.envs.register(id='MECSCH-v2',entry_point=MECSCHEnvV2,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':True,'NOMA_Scheme':True,'OMA_Scheme':False,'Reduction':False,\n",
    "                 'Round_robin':False,'semi_static':False,'heuristic':False})\n",
    "    \n",
    "    # 'MECSCH-v3': Mobile edge computing full computation offloading semi-static NOMA \n",
    "    gym.envs.register(id='MECSCH-v3',entry_point=MECSCHEnvV2,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':True,'NOMA_Scheme':True,'OMA_Scheme':False,'Reduction':False,\n",
    "                 'Round_robin':False,'semi_static':True,'heuristic':False})\n",
    "        \n",
    "    # 'MECSCH-v4': Mobile edge computing full computation offloading round-robin NOMA \n",
    "    gym.envs.register(id='MECSCH-v4',entry_point=MECSCHEnvV2,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':True,'NOMA_Scheme':True,'OMA_Scheme':False,'Reduction':False,\n",
    "                 'Round_robin':True,'semi_static':False,'heuristic':False})\n",
    "     \n",
    "    # 'MECSCH-v5': Mobile edge computing full computation offloading heuristic NOMA \n",
    "    gym.envs.register(id='MECSCH-v5',entry_point=MECSCHEnvV2,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':True,'NOMA_Scheme':True,'OMA_Scheme':False,'Reduction':False,\n",
    "                 'Round_robin':False,'semi_static':False,'heuristic':True})\n",
    "       \n",
    "    # 'MECSCH-v6': Mobile edge computing full computation offloading contention-free/contention-based NOMA \n",
    "    gym.envs.register(id='MECSCH-v6',entry_point=MECSCHEnvV1,\n",
    "         kwargs={'n_ues':8,'n_channels':3,'UE_buffer_capacity':3,'UE_CPU':1e9,'BS_CPU':120e9,'BS_BW':60e6,'hist_msg':1,\n",
    "                 'hist_comm':1,'hist_obs':1,'hist_act':1,'n_voc_ul':2,'n_voc_dl':5,'arrival_prob':1,'max_iters':30,\n",
    "                 'reward_com':1,'penality':-1,'silent':False,'NOMA_Scheme':True,'OMA_Scheme':False})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a3ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contention_free_V2 import BaseStation01, UEAgent01\n",
    "def base_runner(env_id, n_episodes=1000, eval_every=10, n_eval_episodes=10, max_ep_len=25, seed=1024):    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    n_ues = env.n_ues\n",
    "    n_channels = env.n_channels\n",
    "    hist_msg = env.hist_msg\n",
    "    hist_obs = env.hist_obs\n",
    "    n_voc_ul = env.n_voc_ul\n",
    "    n_voc_dl = env.n_voc_dl\n",
    "        \n",
    "    agents = [BaseStation01(n_ues,n_channels,hist_msg,hist_obs,n_voc_ul)]\n",
    "    for ii in range(n_ues):\n",
    "        agents.append(UEAgent01(n_channels,hist_msg,hist_obs,n_voc_dl))\n",
    "\n",
    "    for ii, ag in enumerate(agents):\n",
    "        ag.seed(seed + ii)\n",
    "        \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes)):\n",
    "        done, terminal = False, False\n",
    "        ep_reward,ep_len,ep_droprate,ep_failed = 0,0,0,0\n",
    "        ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0     \n",
    "        obs = env.reset()\n",
    "        for ag in agents:\n",
    "            ag.reset()\n",
    "    \n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = [ag.act(obs[ii]) for ii, ag in enumerate(agents)]\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            done = all(dones)\n",
    "            terminal = ep_len >= env.max_iters\n",
    "            obs = next_obs          \n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    " \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "        \n",
    "        if ep % eval_every == 0:\n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "     \n",
    "    #eval_rewards = np.mean(np.array(train_rewards).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_success_tasks = np.mean(np.array(train_success_tasks).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_success = np.mean(np.array(train_channel_success).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_collision = np.mean(np.array(train_channel_collision).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_idle = np.mean(np.array(train_channel_idle).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_goodput = np.mean(np.array(train_goodput).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_droprate = np.mean(np.array(train_droprate).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_failed = np.mean(np.array(train_failed).reshape(-1,eval_every), axis=1).tolist()\n",
    "   \n",
    "    return train_rewards,train_success_tasks,train_channel_success,train_channel_collision,train_channel_idle,\\\n",
    "           train_goodput,train_droprate,train_failed,eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,\\\n",
    "           eval_channel_idle,eval_goodput,eval_droprate,eval_failed           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bc93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contention_based_V2 import BaseStation02, UEAgent02\n",
    "def contention_base_runner(env_id, n_episodes=1000, eval_every=10, n_eval_episodes=10, max_ep_len=25, seed=1024):    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    n_ues = env.n_ues\n",
    "    n_channels = env.n_channels\n",
    "    hist_msg = env.hist_msg\n",
    "    hist_obs = env.hist_obs\n",
    "    n_voc_ul = env.n_voc_ul\n",
    "    n_voc_dl = env.n_voc_dl\n",
    "        \n",
    "    agents = [BaseStation02(n_ues,n_channels,hist_msg,hist_obs,n_voc_ul)]\n",
    "    for ii in range(n_ues):\n",
    "        agents.append(UEAgent02(n_channels,hist_msg,hist_obs,n_voc_dl))\n",
    "\n",
    "    for ii, ag in enumerate(agents):\n",
    "        ag.seed(seed + ii)\n",
    "            \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes)):\n",
    "        done, terminal = False, False\n",
    "        ep_reward,ep_len,ep_droprate,ep_failed= 0,0,0,0\n",
    "        ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0     \n",
    "        obs = env.reset()\n",
    "        for ag in agents:\n",
    "            ag.reset()\n",
    "    \n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = [ag.act(obs[ii]) for ii, ag in enumerate(agents)]\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            done = all(dones)\n",
    "            terminal = ep_len >= env.max_iters\n",
    "            obs = next_obs\n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    "           \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "        \n",
    "        if ep % eval_every == 0:\n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "    \n",
    "    #eval_rewards = np.mean(np.array(train_rewards).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_success_tasks = np.mean(np.array(train_success_tasks).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_success = np.mean(np.array(train_channel_success).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_collision = np.mean(np.array(train_channel_collision).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_idle = np.mean(np.array(train_channel_idle).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_goodput = np.mean(np.array(train_goodput).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_droprate = np.mean(np.array(train_droprate).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_failed = np.mean(np.array(train_failed).reshape(-1,eval_every), axis=1).tolist()\n",
    "    \n",
    "    return train_rewards,train_success_tasks,train_channel_success,train_channel_collision,train_channel_idle,train_goodput,\\\n",
    "           train_droprate,train_failed,eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,\\\n",
    "           eval_channel_idle,eval_goodput,eval_droprate,eval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c63d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semi_static import BaseStation03\n",
    "def semi_static_runner(env_id,n_episodes=1000,eval_every=10,n_eval_episodes=10,max_ep_len=25,seed=1024):    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    n_ues = env.n_ues\n",
    "    n_channels = env.n_channels\n",
    "    hist_msg = env.hist_msg\n",
    "    hist_obs = env.hist_obs\n",
    "    n_voc_ul = env.n_voc_ul\n",
    "    n_voc_dl = env.n_voc_dl\n",
    "    n_actions = env.BSnA\n",
    "        \n",
    "    agents = [BaseStation03(n_ues,n_channels,hist_msg,hist_obs,n_voc_ul,n_actions)]\n",
    "\n",
    "    for ii, ag in enumerate(agents):\n",
    "        ag.seed(seed + ii)\n",
    "            \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes)):\n",
    "        done, terminal = False, False\n",
    "        ep_reward,ep_len,ep_droprate,ep_failed = 0,0,0,0\n",
    "        ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0      \n",
    "        obs = env.reset()\n",
    "        for ag in agents:\n",
    "            ag.reset()\n",
    "    \n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = [ag.act(obs[ii]) for ii, ag in enumerate(agents)]\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            done = all(dones)\n",
    "            terminal = ep_len >= env.max_iters\n",
    "            obs = next_obs\n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    "        \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "        \n",
    "        if ep % eval_every == 0:\n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "    \n",
    "    #eval_rewards = np.mean(np.array(train_rewards).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_success_tasks = np.mean(np.array(train_success_tasks).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_success = np.mean(np.array(train_channel_success).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_collision = np.mean(np.array(train_channel_collision).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_idle = np.mean(np.array(train_channel_idle).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_goodput = np.mean(np.array(train_goodput).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_droprate = np.mean(np.array(train_droprate).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_failed = np.mean(np.array(train_failed).reshape(-1,eval_every), axis=1).tolist()\n",
    "\n",
    "    \n",
    "    return train_rewards, train_success_tasks,train_channel_success,train_channel_collision,train_channel_idle,\\\n",
    "           train_goodput,train_droprate,train_failed,eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,\\\n",
    "           eval_channel_idle,eval_goodput,eval_droprate,eval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3387eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Round_robin import BaseStation04\n",
    "def Round_robin_runner(env_id,n_episodes=1000,eval_every=10,n_eval_episodes=10,max_ep_len=25,seed=1024):    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    n_ues = env.n_ues\n",
    "    n_channels = env.n_channels\n",
    "    hist_msg = env.hist_msg\n",
    "    hist_obs = env.hist_obs\n",
    "    n_voc_ul = env.n_voc_ul\n",
    "        \n",
    "    agents = [BaseStation04(n_ues,n_channels,hist_msg,hist_obs,n_voc_ul)]\n",
    "\n",
    "    for ii, ag in enumerate(agents):\n",
    "        ag.seed(seed + ii)\n",
    "            \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes)):\n",
    "        done, terminal = False, False\n",
    "        ep_reward,ep_len,ep_droprate,ep_failed = 0,0,0,0\n",
    "        ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0   \n",
    "        obs = env.reset()\n",
    "        for ag in agents:\n",
    "            ag.reset()\n",
    "    \n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = [ag.act(obs[ii],ep_len) for ii, ag in enumerate(agents)]\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            done = all(dones)\n",
    "            terminal = ep_len >= env.max_iters\n",
    "            obs = next_obs\n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    "         \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "        \n",
    "        if ep % eval_every == 0:    \n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "    \n",
    "    #eval_rewards = np.mean(np.array(train_rewards).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_success_tasks = np.mean(np.array(train_success_tasks).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_success = np.mean(np.array(train_channel_success).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_collision = np.mean(np.array(train_channel_collision).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_idle = np.mean(np.array(train_channel_idle).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_goodput = np.mean(np.array(train_goodput).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_droprate = np.mean(np.array(train_droprate).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_failed = np.mean(np.array(train_failed).reshape(-1,eval_every), axis=1).tolist()\n",
    "    \n",
    "    return train_rewards,train_success_tasks,train_channel_success,train_channel_collision,train_channel_idle,\\\n",
    "           train_goodput,train_droprate,train_failed,eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,\\\n",
    "           eval_channel_idle,eval_goodput,eval_droprate,eval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3944dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heuristics import BaseStation05\n",
    "def heuristics_runner(env_id,n_episodes=1000,eval_every=10,n_eval_episodes=10,max_ep_len=25,seed=1024):    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    n_ues = env.n_ues\n",
    "    n_channels = env.n_channels    \n",
    "    n_actions = env.BSnA\n",
    "    Channel_Matrix = env.Channel_Matrix\n",
    "    UE_Power = env.UE_Power\n",
    "    Noise_Power = env.Noise_Power\n",
    "    BS_BW = env.BS_BW\n",
    "    BS_CPU = env.BS_CPU\n",
    "    uplink_th = env.uplink_th\n",
    "    \n",
    "    agents = [BaseStation05(n_ues,n_channels,n_actions,Channel_Matrix,UE_Power,Noise_Power,BS_BW,BS_CPU,uplink_th)]\n",
    "\n",
    "    for ii, ag in enumerate(agents):\n",
    "        ag.seed(seed + ii)\n",
    "            \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in tqdm(range(n_episodes)):\n",
    "        done, terminal = False, False\n",
    "        ep_reward,ep_len,ep_droprate,ep_failed = 0,0,0,0\n",
    "        ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0      \n",
    "        obs = env.reset()\n",
    "        for ag in agents:\n",
    "            ag.reset()\n",
    "    \n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = [ag.act(obs[ii]) for ii, ag in enumerate(agents)]\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            done = all(dones)\n",
    "            terminal = ep_len >= env.max_iters\n",
    "            obs = next_obs\n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    "        \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "            \n",
    "        if ep % eval_every == 0:\n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "    \n",
    "    #eval_rewards = np.mean(np.array(train_rewards).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_success_tasks = np.mean(np.array(train_success_tasks).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_success = np.mean(np.array(train_channel_success).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_collision = np.mean(np.array(train_channel_collision).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_channel_idle = np.mean(np.array(train_channel_idle).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_goodput = np.mean(np.array(train_goodput).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_droprate = np.mean(np.array(train_droprate).reshape(-1,eval_every), axis=1).tolist()\n",
    "    #eval_failed = np.mean(np.array(train_failed).reshape(-1,eval_every), axis=1).tolist()\n",
    "\n",
    "    return train_rewards,train_success_tasks,train_channel_success,train_channel_collision,train_channel_idle,\\\n",
    "           train_goodput,train_droprate,train_failed,eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,\\\n",
    "           eval_channel_idle,eval_goodput,eval_droprate,eval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6415a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agents,env_id=\"MECSCH-v0\",n_episodes=10,max_ep_len=25):\n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    env.seed(1234)\n",
    "       \n",
    "    eval_rewards = []\n",
    "    eval_success_tasks = []\n",
    "    eval_channel_success = []\n",
    "    eval_channel_collision = []\n",
    "    eval_channel_idle = []\n",
    "    eval_goodput = []\n",
    "    eval_droprate = []\n",
    "    eval_failed = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        ep_reward, ep_len, ep_droprate, ep_failed,ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0,0,0,0,0  \n",
    "        done, terminal = False, False\n",
    "        while not (done or terminal):\n",
    "            ep_len += 1\n",
    "            actions = agents.act(obs,explore=False)\n",
    "            next_obs, rewards, dones, info = env.step(actions)\n",
    "            terminal = ep_len > max_ep_len\n",
    "            done = all(dones)\n",
    "            obs = next_obs\n",
    "            ep_reward += np.mean(rewards)\n",
    "            ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "            ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "            ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "            ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "            ep_goodput += info[\"Goodput\"]\n",
    "            ep_droprate += info[\"Drop Rate\"]\n",
    "            ep_failed += info[\"Failed\"]\n",
    "            \n",
    "        eval_rewards.append(ep_reward)\n",
    "        eval_success_tasks.append(ep_success_tasks)\n",
    "        eval_channel_success.append(ep_channel_success/ep_len)\n",
    "        eval_channel_collision.append(ep_channel_collision/ep_len)\n",
    "        eval_channel_idle.append(ep_channel_idle/ep_len)\n",
    "        eval_goodput.append(ep_goodput/ep_len)\n",
    "        eval_droprate.append(ep_droprate)\n",
    "        eval_failed.append(ep_failed)\n",
    "        \n",
    "    return eval_rewards,eval_success_tasks,eval_channel_success,eval_channel_collision,eval_channel_idle,eval_goodput,\\\n",
    "           eval_droprate,eval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc4bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_on(env_id=\"MECSCH-v0\",n_episodes=1000,max_ep_len=25,parameter_sharing=False,seed=1024,disable_tqdm=False,\n",
    "               eval_every=10,n_eval_episodes=10,**kwargs):\n",
    "    \n",
    "    register_envs()\n",
    "    env = gym.make(env_id)\n",
    "    agents = OnPolicyWrapper(env,parameter_sharing=parameter_sharing,**kwargs)\n",
    "    env.seed(seed)\n",
    "    seed_everything(seed)    \n",
    "    batch_size = kwargs.get(\"batch_size\",64)\n",
    "    \n",
    "    train_rewards = []    \n",
    "    train_success_tasks = []\n",
    "    train_channel_success = []\n",
    "    train_channel_collision = []\n",
    "    train_channel_idle = []\n",
    "    train_goodput = []\n",
    "    train_droprate = []\n",
    "    train_failed = []\n",
    "    \n",
    "    evals_rewards = []\n",
    "    evals_success_tasks = []\n",
    "    evals_channel_success = []\n",
    "    evals_channel_collision = []\n",
    "    evals_channel_idle = []\n",
    "    evals_goodput = []\n",
    "    evals_droprate = []\n",
    "    evals_failed = []\n",
    "    total_count = 0\n",
    "    \n",
    "    with tqdm(total=n_episodes,desc=\"Training\",disable=disable_tqdm) as pbar:\n",
    "        for ep in range(n_episodes):\n",
    "            obs = env.reset()\n",
    "            ep_reward,ep_len,ep_droprate,ep_failed,ep_success_tasks,ep_channel_success,ep_channel_collision,ep_channel_idle,ep_goodput = 0,0,0,0,0,0,0,0,0     \n",
    "            done, terminal = False, False\n",
    "            while not (done or terminal):\n",
    "                ep_len += 1\n",
    "                total_count += 1\n",
    "                actions = agents.act(obs)\n",
    "                values = agents.estimate_value(obs)\n",
    "                next_obs, rewards, dones, info = env.step(actions)               \n",
    "                terminal = ep_len > max_ep_len\n",
    "                agents.experience(ep, obs, actions, rewards, next_obs, dones, values)\n",
    "                done = all(dones)\n",
    "                if total_count >= batch_size:\n",
    "                    next_values = agents.estimate_value(next_obs)\n",
    "                    critic_loss, policy_loss = agents.update(next_values)\n",
    "                    total_count = 0\n",
    "                obs = next_obs\n",
    "                ep_reward += np.mean(rewards)\n",
    "                ep_success_tasks += info[\"No. of Success Tasks\"]\n",
    "                ep_channel_success += info[\"Channel Access Success Rate\"]\n",
    "                ep_channel_collision += info[\"Channel Access Collision Rate\"]\n",
    "                ep_channel_idle += info[\"Channel Idle Rate\"]\n",
    "                ep_goodput += info[\"Goodput\"]\n",
    "                ep_droprate += info[\"Drop Rate\"]\n",
    "                ep_failed += info[\"Failed\"]\n",
    "                \n",
    "            pbar.set_postfix({\"episode\": ep+1,\"Training reward\": np.round(ep_reward, decimals=2)})\n",
    "            pbar.update(1)\n",
    "            \n",
    "            train_rewards.append(ep_reward)\n",
    "            train_success_tasks.append(ep_success_tasks)\n",
    "            train_channel_success.append(ep_channel_success/ep_len)\n",
    "            train_channel_collision.append(ep_channel_collision/ep_len)\n",
    "            train_channel_idle.append(ep_channel_idle/ep_len)\n",
    "            train_goodput.append(ep_goodput/ep_len)\n",
    "            train_droprate.append(ep_droprate)\n",
    "            train_failed.append(ep_failed)\n",
    "            \n",
    "            if ep % eval_every == 0:\n",
    "                eval_reward,eval_success_tasks,eval_channel_success,eval_channel_collision,eval_channel_idle,eval_goodput,eval_droprate,eval_failed = test_agents(agents, env_id, n_episodes=n_eval_episodes, max_ep_len=max_ep_len)\n",
    "                evals_rewards.append(np.mean(eval_reward))\n",
    "                evals_success_tasks.append(np.mean(eval_success_tasks))\n",
    "                evals_channel_success.append(np.mean(eval_channel_success))\n",
    "                evals_channel_collision.append(np.mean(eval_channel_collision))\n",
    "                evals_channel_idle.append(np.mean(eval_channel_idle))\n",
    "                evals_goodput.append(np.mean(eval_goodput))\n",
    "                evals_droprate.append(np.mean(eval_droprate))\n",
    "                evals_failed.append(np.mean(eval_failed))\n",
    "                 \n",
    "    feval_reward,feval_success_tasks,feval_channel_success,feval_channel_collision,feval_channel_idle,feval_goodput,feval_droprate,feval_failed = test_agents(agents, env_id, n_episodes=n_eval_episodes, max_ep_len=max_ep_len)\n",
    "\n",
    "    return evals_rewards,evals_success_tasks,evals_channel_success,evals_channel_collision,evals_channel_idle,evals_goodput,evals_droprate,evals_failed,\\\n",
    "           feval_reward,feval_success_tasks,feval_channel_success,feval_channel_collision,feval_channel_idle,feval_goodput,feval_droprate,feval_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ef368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/users/smostafa/Journal_MEC_UseCase_13_11_2023_[32]/models.py:162: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  actions = torch.stack([d.sample() for d in dists]).T\n",
      "/users/smostafa/Journal_MEC_UseCase_13_11_2023_[32]/models.py:162: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  actions = torch.stack([d.sample() for d in dists]).T\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v1\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v2\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v3\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v4\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v5\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v6\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v1\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v2\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v3\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v4\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v5\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/users/smostafa/.local/lib/python3.10/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment MECSCH-v6\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 12.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 12.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages/numpy/lib/arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n",
      "/CSC_CONTAINER/miniconda/envs/env1/lib/python3.10/site-packages/numpy/lib/arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n",
      "/users/smostafa/Journal_MEC_UseCase_13_11_2023_[32]/envs/mecsch_v2.py:455: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(action)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2001 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/2001 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "envs = [\"MECSCH-v1\",\"MECSCH-v2\"]\n",
    "labels = [\"Proposed with Reduction\",\"Proposed No-Reduction\"]\n",
    "results_mappo = {}\n",
    "\n",
    "for env,label in zip(envs,labels):\n",
    "    env_id = env\n",
    "    n_episodes = 2001\n",
    "    max_ep_len = 15\n",
    "    n_seeds = 2\n",
    "    eval_every = 100\n",
    "    n_eval_episodes = 2001\n",
    "    parameter_sharing = False\n",
    "    disable_tqdm = False\n",
    "\n",
    "    mappo_params = {\"actor_lr\":1e-3,\"critic_lr\":1e-3,\"gamma\": 0.99,\"gae\": True,\"gae_lmb\":0.95,\"shuffle\": False,\"model\":\"MAPPO\"}\n",
    "    list_seeds = 1024 * np.arange(1, n_seeds+1)\n",
    "    # Results for MAPPO \n",
    "    mappo_params[\"local_critic\"] = True\n",
    "    results_mappo[label] = Parallel(n_jobs=-1,verbose=10)(delayed(run_sim_on)(env_id,n_episodes,max_ep_len,parameter_sharing,seed, disable_tqdm, eval_every, n_eval_episodes, **mappo_params) for seed in list_seeds.tolist())\n",
    "\n",
    "results_mappo[\"Semi-static\"] = Parallel(n_jobs=-1,verbose=10)(delayed(semi_static_runner)(\"MECSCH-v3\",n_episodes=n_episodes,eval_every=eval_every,n_eval_episodes=n_eval_episodes,max_ep_len=max_ep_len,seed=seed) for seed in list_seeds.tolist())\n",
    "results_mappo[\"Round-robin\"] = Parallel(n_jobs=-1,verbose=10)(delayed(Round_robin_runner)(\"MECSCH-v4\",n_episodes=n_episodes,eval_every=eval_every,n_eval_episodes=n_eval_episodes,max_ep_len=max_ep_len,seed=seed) for seed in list_seeds.tolist())\n",
    "results_mappo[\"Heuristics\"] = Parallel(n_jobs=-1,verbose=10)(delayed(heuristics_runner)(\"MECSCH-v5\",n_episodes=n_episodes,eval_every=eval_every,n_eval_episodes=n_eval_episodes,max_ep_len=max_ep_len,seed=seed) for seed in list_seeds.tolist())\n",
    "results_mappo[\"Contention-free\"] = Parallel(n_jobs=-1,verbose=10)(delayed(base_runner)(\"MECSCH-v6\",n_episodes=n_episodes,eval_every=eval_every,n_eval_episodes=n_eval_episodes, max_ep_len=max_ep_len, seed=seed) for seed in list_seeds.tolist())\n",
    "results_mappo[\"Contention-based\"] = Parallel(n_jobs=-1,verbose=10)(delayed(contention_base_runner)(\"MECSCH-v6\",n_episodes=n_episodes, eval_every=eval_every,n_eval_episodes=n_eval_episodes, max_ep_len=max_ep_len, seed=seed) for seed in list_seeds.tolist())\n",
    "\n",
    "results = [results_mappo[\"Proposed with Reduction\"],results_mappo[\"Proposed No-Reduction\"],results_mappo[\"Semi-static\"],\\\n",
    "           results_mappo[\"Round-robin\"],results_mappo[\"Heuristics\"],results_mappo[\"Contention-free\"],results_mappo[\"Contention-based\"]]\n",
    "\n",
    "schemes = [\"Proposed with Reduction\",\"Proposed No-Reduction\",\"Semi-static\",\"Round-robin\",\"Heuristics\",\"Contention-free\",\\\n",
    "           \"Contention-based\"]\n",
    "\n",
    "eval_reward_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]} \n",
    "eval_success_tasks_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_channel_success_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_channel_collision_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_channel_idle_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_goodput_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_URLLC_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_droprate_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}\n",
    "eval_failed_results = {\"Proposed with Reduction\":[],\"Proposed No-Reduction\":[],\"Contention-free\":[],\"Contention-based\":[],\"Semi-static\":[],\"Round-robin\":[],\"Heuristics\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebff6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = {\"episodes\":np.arange(0,n_episodes,eval_every,dtype=np.int16)}\n",
    "\n",
    "for number, result in enumerate(list_seeds):    \n",
    "    Seed_0_reward = pd.DataFrame(data=data_0)\n",
    "    Seed_0_Success_Task = pd.DataFrame(data=data_0)\n",
    "    Seed_0_channel_success = pd.DataFrame(data=data_0) \n",
    "    Seed_0_channel_collision = pd.DataFrame(data=data_0)\n",
    "    Seed_0_channel_idle = pd.DataFrame(data=data_0) \n",
    "    Seed_0_goodput = pd.DataFrame(data=data_0)\n",
    "    Seed_0_droprate = pd.DataFrame(data=data_0)\n",
    "    Seed_0_failed = pd.DataFrame(data=data_0)\n",
    "    \n",
    "    Seed_1_reward = pd.DataFrame(data=data_0)\n",
    "    Seed_1_Success_Task = pd.DataFrame(data=data_0)\n",
    "    Seed_1_channel_success = pd.DataFrame(data=data_0) \n",
    "    Seed_1_channel_collision = pd.DataFrame(data=data_0) \n",
    "    Seed_1_channel_idle = pd.DataFrame(data=data_0) \n",
    "    Seed_1_goodput = pd.DataFrame(data=data_0)\n",
    "    Seed_1_droprate = pd.DataFrame(data=data_0)\n",
    "    Seed_1_failed = pd.DataFrame(data=data_0)\n",
    "    \n",
    "    Seed_2_reward = pd.DataFrame(data=data_0)\n",
    "    Seed_2_Success_Task = pd.DataFrame(data=data_0)\n",
    "    Seed_2_channel_success = pd.DataFrame(data=data_0) \n",
    "    Seed_2_channel_collision = pd.DataFrame(data=data_0) \n",
    "    Seed_2_channel_idle = pd.DataFrame(data=data_0) \n",
    "    Seed_2_goodput = pd.DataFrame(data=data_0)\n",
    "    Seed_2_droprate = pd.DataFrame(data=data_0)\n",
    "    Seed_2_failed = pd.DataFrame(data=data_0)\n",
    "    \n",
    "    Seed_3_reward = pd.DataFrame(data=data_0)\n",
    "    Seed_3_Success_Task = pd.DataFrame(data=data_0)\n",
    "    Seed_3_channel_success = pd.DataFrame(data=data_0) \n",
    "    Seed_3_channel_collision = pd.DataFrame(data=data_0) \n",
    "    Seed_3_channel_idle = pd.DataFrame(data=data_0) \n",
    "    Seed_3_goodput = pd.DataFrame(data=data_0)\n",
    "    Seed_3_droprate = pd.DataFrame(data=data_0)\n",
    "    Seed_3_failed = pd.DataFrame(data=data_0)\n",
    "    \n",
    "    Seed_4_reward = pd.DataFrame(data=data_0)\n",
    "    Seed_4_Success_Task = pd.DataFrame(data=data_0)\n",
    "    Seed_4_channel_success = pd.DataFrame(data=data_0) \n",
    "    Seed_4_channel_collision = pd.DataFrame(data=data_0) \n",
    "    Seed_4_channel_idle = pd.DataFrame(data=data_0) \n",
    "    Seed_4_goodput = pd.DataFrame(data=data_0)\n",
    "    Seed_4_droprate = pd.DataFrame(data=data_0)\n",
    "    Seed_4_failed = pd.DataFrame(data=data_0)\n",
    "    \n",
    "for key, results_ind in zip(schemes, results):\n",
    "    eval_reward_results[key] = np.mean([np.array(result[8]) for result in results_ind],axis=0) \n",
    "    eval_success_tasks_results[key] = np.mean([np.array(result[9]) for result in results_ind],axis=0)  \n",
    "    eval_channel_success_results[key] = np.mean([np.array(result[10]) for result in results_ind],axis=0)      \n",
    "    eval_channel_collision_results[key] = np.mean([np.array(result[11]) for result in results_ind],axis=0)  \n",
    "    eval_channel_idle_results[key] = np.mean([np.array(result[12]) for result in results_ind],axis=0)  \n",
    "    eval_goodput_results[key] = np.mean([np.array(result[13]) for result in results_ind],axis=0)\n",
    "    eval_droprate_results[key] = np.mean([np.array(result[14]) for result in results_ind],axis=0)\n",
    "    eval_failed_results[key] = np.mean([np.array(result[15]) for result in results_ind],axis=0)\n",
    "    \n",
    "    for number, result in enumerate(results_ind):\n",
    "        if number == 0:\n",
    "            Seed_0_reward[key] = window_mean(np.array(result[0]),10) \n",
    "            Seed_0_Success_Task[key] = window_mean(np.array(result[1]), 10) \n",
    "            Seed_0_channel_success[key] = window_mean(np.array(result[2]),10) \n",
    "            Seed_0_channel_collision[key] = window_mean(np.array(result[3]),10) \n",
    "            Seed_0_channel_idle[key] = window_mean(np.array(result[4]),10)    \n",
    "            Seed_0_goodput[key] = window_mean(np.array(result[5]),10) \n",
    "            Seed_0_droprate[key] = window_mean(np.array(result[6]),10) \n",
    "            Seed_0_failed[key] = window_mean(np.array(result[7]),10) \n",
    "        elif number == 1:\n",
    "            Seed_1_reward[key] = window_mean(np.array(result[0]),10) \n",
    "            Seed_1_Success_Task[key] = window_mean(np.array(result[1]), 10) \n",
    "            Seed_1_channel_success[key] = window_mean(np.array(result[2]),100) \n",
    "            Seed_1_channel_collision[key] = window_mean(np.array(result[3]),10) \n",
    "            Seed_1_channel_idle[key] = window_mean(np.array(result[4]),10)    \n",
    "            Seed_1_goodput[key] = window_mean(np.array(result[5]),10)\n",
    "            Seed_1_droprate[key] = window_mean(np.array(result[6]),10)\n",
    "            Seed_1_failed[key] = window_mean(np.array(result[7]),10) \n",
    "        elif number == 2:\n",
    "            Seed_2_reward[key] = window_mean(np.array(result[0]),10) \n",
    "            Seed_2_Success_Task[key] = window_mean(np.array(result[1]), 10) \n",
    "            Seed_2_channel_success[key] = window_mean(np.array(result[2]),10) \n",
    "            Seed_2_channel_collision[key] = window_mean(np.array(result[3]),10) \n",
    "            Seed_2_channel_idle[key] = window_mean(np.array(result[4]),10) \n",
    "            Seed_2_goodput[key] = window_mean(np.array(result[5]),10) \n",
    "            Seed_2_droprate[key] = window_mean(np.array(result[6]),10) \n",
    "            Seed_2_failed[key] = window_mean(np.array(result[7]),10) \n",
    "        elif number == 3:\n",
    "            Seed_3_reward[key] = window_mean(np.array(result[0]),10) \n",
    "            Seed_3_Success_Task[key] = window_mean(np.array(result[1]), 10) \n",
    "            Seed_3_channel_success[key] = window_mean(np.array(result[2]),10) \n",
    "            Seed_3_channel_collision[key] = window_mean(np.array(result[3]),10)\n",
    "            Seed_3_channel_idle[key] = window_mean(np.array(result[4]),10) \n",
    "            Seed_3_goodput[key] = window_mean(np.array(result[5]),10) \n",
    "            Seed_3_droprate[key] = window_mean(np.array(result[6]),10) \n",
    "            Seed_3_failed[key] = window_mean(np.array(result[7]),10) \n",
    "        elif number == 4:\n",
    "            Seed_4_reward[key] = window_mean(np.array(result[0]),10) \n",
    "            Seed_4_Success_Task[key] = window_mean(np.array(result[1]), 10) \n",
    "            Seed_4_channel_success[key] = window_mean(np.array(result[2]),10) \n",
    "            Seed_4_channel_collision[key] = window_mean(np.array(result[3]),10)  \n",
    "            Seed_4_channel_idle[key] = window_mean(np.array(result[4]),10) \n",
    "            Seed_4_goodput[key] = window_mean(np.array(result[5]),10) \n",
    "            Seed_4_droprate[key] = window_mean(np.array(result[6]),10) \n",
    "            Seed_4_failed[key] = window_mean(np.array(result[7]),10) \n",
    "\n",
    "train_reward = pd.concat([Seed_0_reward,Seed_1_reward,Seed_2_reward,Seed_3_reward,Seed_4_reward], axis=0)\n",
    "train_reward.to_csv('train_Reward_Data.csv',index=False)\n",
    "\n",
    "train_success_tasks = pd.concat([Seed_0_Success_Task,Seed_1_Success_Task,Seed_2_Success_Task,Seed_3_Success_Task,Seed_4_Success_Task], axis=0)\n",
    "train_success_tasks.to_csv('train_success_tasks_Data.csv',index=False)\n",
    "\n",
    "train_channel_success = pd.concat([Seed_0_channel_success,Seed_1_channel_success,Seed_2_channel_success,Seed_3_channel_success,Seed_4_channel_success], axis=0)\n",
    "train_channel_success.to_csv('train_channel_success_Data.csv',index=False)\n",
    "\n",
    "train_channel_collision = pd.concat([Seed_0_channel_collision,Seed_1_channel_collision,Seed_2_channel_collision,Seed_3_channel_collision,Seed_4_channel_collision], axis=0)\n",
    "train_channel_collision.to_csv('train_channel_collision_Data.csv',index=False)\n",
    "\n",
    "train_channel_idle = pd.concat([Seed_0_channel_idle,Seed_1_channel_idle,Seed_2_channel_idle,Seed_3_channel_idle,Seed_4_channel_idle], axis=0)\n",
    "train_channel_idle.to_csv('train_channel_idle_Data.csv',index=False)\n",
    "\n",
    "train_goodput = pd.concat([Seed_0_goodput,Seed_1_goodput,Seed_2_goodput,Seed_3_goodput,Seed_4_goodput], axis=0)\n",
    "train_goodput.to_csv('train_goodput_Data.csv',index=False)\n",
    "\n",
    "train_droprate = pd.concat([Seed_0_droprate,Seed_1_droprate,Seed_2_droprate,Seed_3_droprate,Seed_4_droprate], axis=0)\n",
    "train_droprate.to_csv('train_droprate_Data.csv',index=False)\n",
    "\n",
    "train_failed = pd.concat([Seed_0_failed,Seed_1_failed,Seed_2_failed,Seed_3_failed,Seed_4_failed], axis=0)\n",
    "train_failed.to_csv('train_failed_Data.csv',index=False)\n",
    "\n",
    "eval_reward = pd.DataFrame.from_dict(eval_reward_results)\n",
    "eval_reward.to_csv('eval_Reward_Data.csv',index=False)\n",
    "\n",
    "eval_success_tasks = pd.DataFrame.from_dict(eval_success_tasks_results)\n",
    "eval_success_tasks.to_csv('eval_success_tasks_Data.csv',index=False)\n",
    "\n",
    "eval_channel_success = pd.DataFrame.from_dict(eval_channel_success_results)\n",
    "eval_channel_success.to_csv('eval_channel_success_Data.csv',index=False)\n",
    "\n",
    "eval_channel_collision = pd.DataFrame.from_dict(eval_channel_collision_results)\n",
    "eval_channel_collision.to_csv('eval_channel_collision_Data.csv',index=False)\n",
    "\n",
    "eval_channel_idle = pd.DataFrame.from_dict(eval_channel_idle_results)\n",
    "eval_channel_idle.to_csv('eval_channel_idle_Data.csv',index=False)\n",
    "\n",
    "eval_goodput = pd.DataFrame.from_dict(eval_goodput_results)\n",
    "eval_goodput.to_csv('eval_goodput_Data.csv',index=False)\n",
    "\n",
    "eval_droprate = pd.DataFrame.from_dict(eval_droprate_results)\n",
    "eval_droprate.to_csv('eval_droprate_Data.csv',index=False)\n",
    "\n",
    "eval_failed = pd.DataFrame.from_dict(eval_failed_results)\n",
    "eval_failed.to_csv('eval_failed_Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00192d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Style:\n",
    "colors = sns.color_palette(n_colors= 15).as_hex()\n",
    "mpl.rcParams.update({\"font.size\": 15, \"axes.labelsize\": 15, \"lines.markersize\": 10})\n",
    "sns.set(rc={'figure.figsize': (8,6)})\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "linestyles = [\"--\", \"-.\",\":\",\"-\",\"--\",\"-.\",\":\",\"-\",\"--\", \"-.\",\":\",\"-\",\"--\",\"-.\",\":\",\"-\"]\n",
    "markers = [\"s\",\"o\",\"d\",\"v\",\"P\",\"<\",\"X\",\"s\",\"o\",\"d\",\"v\",\"P\",\"<\",\"X\"]\n",
    "set_fonts()\n",
    "#colors = set_style()\n",
    "# Define the size and style of the circle to show the mean of the box plots:\n",
    "meanpointprops = dict(marker='o', markeredgecolor='black',markerfacecolor='lightgray', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reward_results = pd.read_csv('train_Reward_Data.csv')\n",
    "train_success_tasks_results = pd.read_csv('train_success_tasks_Data.csv')\n",
    "train_channel_success_results = pd.read_csv('train_channel_success_Data.csv')\n",
    "train_channel_collision_results = pd.read_csv('train_channel_collision_Data.csv')\n",
    "train_channel_idle_results = pd.read_csv('train_channel_idle_Data.csv')\n",
    "train_goodput_results = pd.read_csv('train_goodput_Data.csv')\n",
    "train_droprate_results = pd.read_csv('train_droprate_Data.csv')\n",
    "train_failed_results = pd.read_csv('train_failed_Data.csv')\n",
    "\n",
    "eval_reward_results = pd.read_csv('eval_Reward_Data.csv')\n",
    "eval_success_tasks_results = pd.read_csv('eval_success_tasks_Data.csv')\n",
    "eval_channel_success_results = pd.read_csv('eval_channel_success_Data.csv')\n",
    "eval_channel_collision_results = pd.read_csv('eval_channel_collision_Data.csv')\n",
    "eval_channel_idle_results = pd.read_csv('eval_channel_idle_Data.csv')\n",
    "eval_goodput_results = pd.read_csv('eval_goodput_Data.csv')\n",
    "eval_droprate_results = pd.read_csv('eval_droprate_Data.csv')\n",
    "eval_failed_results = pd.read_csv('eval_failed_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f3fe1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_axis = np.arange(0,n_episodes+1,eval_every) \n",
    "step_plot = int(0.05 * n_episodes)\n",
    "width = int(0.03*n_episodes) # Width of the box plots\n",
    "\n",
    "fig1 = plt.figure(\"Figure 1\")  \n",
    "data_success_tasks = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_reward_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_reward_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_reward_results.items():\n",
    "    data_success_tasks.append(value)\n",
    "for ii, test_data in enumerate(data_success_tasks):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_success_tasks) + 2\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=3, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.1))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.savefig(\"reward.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig2 = plt.figure(\"Figure 2\")  \n",
    "data_success_tasks = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_success_tasks_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_success_tasks_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_success_tasks_results.items():\n",
    "    data_success_tasks.append(value)\n",
    "for ii, test_data in enumerate(data_success_tasks):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_success_tasks) + 2\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.1))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"No. of Successful Tasks\")\n",
    "plt.savefig(\"task.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig3 = plt.figure(\"Figure 3\")\n",
    "data_channel_success = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_channel_success_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_channel_success_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_channel_success_results.items():\n",
    "    data_channel_success.append(value)\n",
    "for ii, test_data in enumerate(data_channel_success):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_success_tasks) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Channels Access Success Rate\")\n",
    "plt.savefig(\"channel_success.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig4 = plt.figure(\"Figure 4\")  \n",
    "data_channel_collision = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_channel_collision_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_channel_collision_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_channel_collision_results.items():\n",
    "    data_channel_collision.append(value)\n",
    "for ii, test_data in enumerate(data_channel_collision):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_success_tasks) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Channels Access Collision Rate\")\n",
    "plt.savefig(\"channel_collision.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig5 = plt.figure(\"Figure 5\")  \n",
    "data_channel_idle = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_channel_idle_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_channel_idle_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_channel_idle_results.items():\n",
    "    data_channel_idle.append(value)\n",
    "for ii, test_data in enumerate(data_channel_idle):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_success_tasks) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Channels Idle Rate\")\n",
    "plt.savefig(\"channel_idle.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig7 = plt.figure(\"Figure 7\")  \n",
    "data_goodput = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_goodput_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_goodput_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_goodput_results.items():\n",
    "    data_goodput.append(value)\n",
    "for ii, test_data in enumerate(data_goodput):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_goodput) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Goodput\")\n",
    "plt.savefig(\"goodput.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig10 = plt.figure(\"Figure 10\")  \n",
    "data_droprate = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_droprate_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_droprate_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_droprate_results.items():\n",
    "    data_droprate.append(value)\n",
    "for ii, test_data in enumerate(data_droprate):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_droprate) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"No. of Packets Dropped\")\n",
    "plt.savefig(\"drop.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig11 = plt.figure(\"Figure 11\")  \n",
    "data_droprate = []\n",
    "y_max_ls =[]\n",
    "i = 0\n",
    "for key, value in train_failed_results.iloc[:,1:10].items():\n",
    "    g = sns.lineplot(data=train_failed_results,x='episodes',y=key,label=key,linestyle=linestyles[i],color=colors[i],marker=markers[i],\n",
    "                     markevery=eval_every, markersize=5)\n",
    "    i +=1    \n",
    "g.axes.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "plt.axvline(n_episodes + 0.7*step_plot, color='k', linestyle=\"--\")\n",
    "plt.draw()\n",
    "locs, labels = plt.xticks() \n",
    "for key, value in eval_failed_results.items():\n",
    "    data_droprate.append(value)\n",
    "for ii, test_data in enumerate(data_droprate):\n",
    "    ymax = draw_boxplot(test_data, color=colors[ii], positions=[n_episodes + (1.5+ii)*step_plot],\n",
    "                        widths=width, showfliers=False, meanprops=meanpointprops, showmeans=True, whis=1.0)\n",
    "    y_max_ls.append(ymax)\n",
    "end_tick = len(data_droprate) + 4\n",
    "plt.xticks(locs[1:-1], labels[1:-1])\n",
    "ylim_ = max(y_max_ls)\n",
    "plt.ylim(-0.02, ylim_)\n",
    "draw_brace(g.axes, [0, n_episodes], ylim_, \"Train\")\n",
    "draw_brace(g.axes, [n_episodes + 0.8*step_plot, n_episodes + end_tick*step_plot], ylim_, \"Test\")\n",
    "x_min, _ = g.axes.get_xlim()\n",
    "plt.xlim(x_min, n_episodes + end_tick*step_plot)\n",
    "plt.legend(ncol=2, frameon=True, shadow=True,bbox_to_anchor=(0.1,1.2))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"No. of Failed Tasks\")\n",
    "plt.savefig(\"failed.jpg\",dpi=600,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f876530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "schemes = [\"Proposed with Reduction\",\"Proposed No-Reduction\",\"Contention-free\",\"Contention-based\",\"Semi-static\",\"Round-robin\",\"Heuristics\"]\n",
    "values = []\n",
    "for key, value in eval_droprate_results.items():\n",
    "    values.append(np.mean(value))    \n",
    "fig , ax = plt.subplots()\n",
    "ax.bar(schemes,values, color=colors)\n",
    "plt.xticks(rotation=-70)\n",
    "ax.set_xlabel('Schemes')\n",
    "ax.set_ylabel(\"No. of Packets Dropped\")\n",
    "plt.savefig(\"drop.pdf\",dpi=1500,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "values = []\n",
    "for key, value in eval_channel_idle_results.items():\n",
    "    values.append(np.mean(value))    \n",
    "fig , ax = plt.subplots()\n",
    "ax.bar(schemes,values, color=colors)\n",
    "plt.xticks(rotation=-70)\n",
    "ax.set_xlabel('Schemes')\n",
    "ax.set_ylabel(\"Channels Idle Rate\")\n",
    "plt.savefig(\"idle.pdf\",dpi=1500,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "values = []\n",
    "for key, value in eval_channel_collision_results.items():\n",
    "    values.append(np.mean(value))    \n",
    "fig , ax = plt.subplots()\n",
    "ax.bar(schemes,values, color=colors)\n",
    "plt.xticks(rotation=-70)\n",
    "ax.set_xlabel('Schemes')\n",
    "ax.set_ylabel(\"Channels Access Collision Rate\")\n",
    "plt.savefig(\"collision.pdf\",dpi=1500,bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
